 * protocol V2 :
    - a new response header to distinguish between not-found and errors as response to
      GET/SET/OFFSET/HEAD commands

    - use a simple crc byte instead of the 8-bytes siphash signature (especially beneficial
      for chunked-signing). This also means that it's going to become a checksum and not a signature anymore  

    - introduce an extended GET command which returns the timestamp and the node responsible
      for the requested key as second and third record of the response (or perhaps some structure
      holding more meta-data as second record of the response)

    - introduce a CAS command

    - extend set interface to allow controlling if the expiry time should be renewed when the key is
      accessed or not (and let it honor the initial expiration time).

    - support message compression

    - introduce GET_MULTI and SET_MULTI commands.
      This will reduce the overhead of creating a sequence of get/set commands and also allow clients
      without knowledge of the complete shard (or always communicating with a single node) to still
      commence set_multi/get_multi commands but relying on the server to take care of parallelizing
      the operation if possible.

    NOTE: V2 implementation must ensure compatibility with V1 clients which, as long as the
          changes are the ones described above, means using the old response header when
          answering to a failing GET/SET/OFFSET/HEAD command.

 * minimize the usage of the NOOP to verify the connection status.
   When connections are reused quickly there is no point in checking the status again,
   so a threshold should be added which controls whether we need to check the status
   or we can avoid since it has been just used

 * server-side get_multi() and set_multi() implementation, as for the client implementation
   server nodes can take care of parallelizing (when possible) batch set/get commands which involve
   keys not owned by the current server.

 * refactor the API actually exposed to set internal shardcache flags and options
   once an instance has been created. The way it's actually implemented is suboptimal
   because adding a new option requires to add both a member to the shardcache_t structure
   and a new function exposed by the shardcache library. Since options and flags are growing
   and there will be probably more in the future, there is already the need of a proper
   API to allow getting/setting internal flags and params and a better way of storing
   those within the shardcache_t structure

 * complete and test the replica support

 * parallelize migrations

 * refactor arc.c to get rid of the recursive lock
   the fetch logic should be taken out of arc_move() and moved to a separated function
   which can then be used also in arc_lookup() when a new object is being created because a new
   key was requested and it wasn't in the cache.
   arc_lookup must try immediately to fetch the key/value from the storage to check 
   if it exists or not ... so that it can fail early without adding a new empty object
   to the hashtable to then fail later at arc_move() when trying to fetch the value.
   The problem with such a change will be finding a way to keep fetching of the object serialized
   so that multiple threads won't try accessing the storage to fetch the same object.
   At the moment this is ensured by locking the actual object (which is inserted immediately
   when a new key is requested , so further threads will lock on that same object).
   If this won't be the case anymore we still need a way to serialize access to the
   storage for the same key, while still allowing parallel access to different keys.

* weight items distinguishing between not owned (but fetched from a peer) and owned to decide 
   if the value needs to be cached or not. 
   At the moment a simple/dumb logic is implemented which basically keep the remote items only
   10% of times (by calling  rand() % 10). Better approaches are possible but it's still unclear
   if they are worth implementing or not.

 * parallelize eviction
   evict commands are very small and can be handled quickly but now it's
   only one thread making this job and it might be not enough in case of
   a big number of delete requests for keys not owned by the
   receiving instance).

