 * make schedule/unschedule expiration an asynchronous operation since it's going to be a log(n) operation
   and we don't want to block the get/set request that triggers it.
   The operation should be pushed to a queue and the expirer thread should take care of executing it
   after running the its iomux

 * make use of a dynamic (configurable) number of threads to run the asynchronous inter-node communication.
   (it might also be determined in function of the configured number of workers)

 * take advantage of pipelining in the inter-node communication.
   When new asyncrhonous requests are queued, instead of requesting a new filedescriptor to forward the
   request to a peer, if an aysnc request is already being running to that peer, just push the new request
   to the same connection.

 * extend the storage API to support fetch_multi and store_multi which can be (optianlly, if exposed by the module)
   used to fulfill get_multi and set_multi requests which need to go down to the storage

 * protocol V2 :
    - a new response header to distinguish between not-found and errors as response to
      GET/SET/OFFSET/HEAD commands

    - use a simple crc byte instead of the 8-bytes siphash signature (especially beneficial
      for chunked-signing). This also means that it's going to become a checksum and not a signature anymore  

    - introduce an extended GET command which returns the timestamp and the node responsible
      for the requested key as second and third record of the response (or perhaps some structure
      holding more meta-data as second record of the response)

    - introduce a CAS command

    - extend set interface to allow controlling if the expiry time should be renewed when the key is
      accessed or not (and let it honor the initial expiration time).

    - support message compression

    - introduce GET_MULTI and SET_MULTI commands.
      This will reduce the overhead of creating a sequence of get/set commands and also allow clients
      without knowledge of the complete shard (or always communicating with a single node) to still
      commence set_multi/get_multi commands but relying on the server to take care of parallelizing
      the operation if possible.

    NOTE: V2 implementation must ensure compatibility with V1 clients which, as long as the
          changes are the ones described above, means using the old response header when
          answering to a failing GET/SET/OFFSET/HEAD command.

 * extend the storage API to allow asynchronous fetches.
   Adding a fetch_async callback to the storage structure would be the easiest. Then arc_ops will
   also expose a fetch_async which will be called by arc_lookup() if used in async mode,
   otherwise the synchronous interface will still be used.

 * complete and test the replica support

 * parallelize migrations

 * parallelize eviction
   evict commands are very small and can be handled quickly but now it's
   only one thread making this job and it might be not enough in case of
   a big number of delete requests for keys not owned by the
   receiving instance).

 * refactor the API actually exposed to set internal shardcache flags and options
   once an instance has been created. The way it's actually implemented is suboptimal
   because adding a new option requires to add both a member to the shardcache_t structure
   and a new function exposed by the shardcache library. Since options and flags are growing
   and there will be probably more in the future, there is already the need of a proper
   API to allow getting/setting internal flags and params and a better way of storing
   those within the shardcache_t structure


